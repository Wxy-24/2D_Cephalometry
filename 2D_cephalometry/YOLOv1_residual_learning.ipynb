{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Cephalometry_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f0cabcad77a2253757f400696d74a8a30c2e4cc9",
        "colab_type": "text",
        "id": "5jmtqQTNi_Nk"
      },
      "source": [
        "# **cephalometry project:**\n",
        "## **2D cephalometry landmark localization with CNN** \n",
        "### *YOLO V1 based on Residual Learning* \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "3ce2b3bf606c3bf06d9f61249f580a55812f9e42",
        "colab_type": "text",
        "id": "doqWzi26i_Nm"
      },
      "source": [
        "# Step 1 : Auxiliary steps\n",
        "\n",
        " importing essential python libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab_type": "code",
        "id": "wQzaGiezi_No",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import cv2\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from math import*\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import LeakyReLU\n",
        "from scipy.ndimage.interpolation import zoom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15CPnrIiOlV_",
        "colab_type": "code",
        "outputId": "ddffcf28-73bf-4bbe-910e-b32162740b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "if os.getcwd()=='/content':\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        base_working_dir = '/content/drive/My Drive'\n",
        "        drive.mount('/content/drive')\n",
        "    except:\n",
        "        base_working_dir = os.getcwd()\n",
        "        pass\n",
        "sys.path.append(base_working_dir)\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "print(base_working_dir)\n",
        "from DL.exercise_functions import *\n",
        "\n",
        "base_working_dir = os.path.join(base_working_dir, 'Skin_Cancer_class')\n",
        "\n",
        "if not os.path.exists(base_working_dir):\n",
        "    os.makedirs(base_working_dir)\n",
        "    "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/bin/bash: google-drive-ocamlfuse: command not found\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c170def1ed6bd1e279dc6d5ae86a95cf6cfd2efb",
        "colab_type": "text",
        "id": "J78Y6OhRi_Nx"
      },
      "source": [
        "# Step 2 : Creating dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "colab_type": "code",
        "id": "vdjg_1sLi_Ny",
        "colab": {}
      },
      "source": [
        "#create dataset\n",
        "train=[]\n",
        "label=[]\n",
        "for i in range(1,301):\n",
        "    img = io.imread(os.path.join(\"drive/My Drive/Cephalometry_2d_final/RawImage/TrainingData\", \"%d_crop.bmp\" % i),)\n",
        "    img=img[0:2176,:]\n",
        "    img = zoom(img, zoom=0.25, order=1)\n",
        "    train.append(img)\n",
        "for j in range(1,301):\n",
        "    f = open(os.path.join(\"drive/My Drive/Cephalometry_2d_final/400_senior\", \"%d.txt\" % j))\n",
        "    fp = open(os.path.join(\"drive/My Drive/Cephalometry_2d_final/400_junior\", \"%d.txt\" % j))\n",
        "    line=f.readlines()\n",
        "    linep=fp.readlines()\n",
        "    y=(int(line[8].split(',')[1])+int(linep[8].split(',')[1])-200)//8\n",
        "    x=(int(line[8].split(',')[0])+int(linep[8].split(',')[0]))//8\n",
        "    coordinate=np.array([x,y])\n",
        "    label.append(coordinate)\n",
        "x_train=np.array(train)\n",
        "y_train=np.array(label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "81603d003c5cea8f6e76a1bb69aa4c29a89eebc8",
        "colab_type": "text",
        "id": "b4tQaw_Zi_N2"
      },
      "source": [
        "# Step 3 :Train-Test split\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "68f34a08751a6e16569818ce8b18d9fca93223ad",
        "colab_type": "code",
        "id": "1UPP0YxYi_N3",
        "colab": {}
      },
      "source": [
        "x_train=np.expand_dims(x_train,axis=3)\n",
        "x_train, x_test, y_train, y_test =train_test_split(x_train,y_train, test_size=0.1, random_state=66)\n",
        "x_train, x_validate, y_train, y_validate =train_test_split(x_train,y_train, test_size=0.11, random_state=66)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T99JnSma8En2"
      },
      "source": [
        "Check the shape of dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0de2d23671ac649a0e20f426fb891a4e8f7e903a",
        "colab_type": "code",
        "id": "85SJAhgii_N7",
        "outputId": "9f9423ec-4b79-46f3-967b-c8915516eb76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(240, 544, 480, 1)\n",
            "(240, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kV6eLQ5yUij",
        "colab_type": "text"
      },
      "source": [
        "define loss function for CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bE6fjw--NOOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def euclidean_distance(y_true, y_pred):\n",
        "  return K.mean(K.sqrt(K.sum(K.square(y_pred-y_true),axis=-1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "759026191d74e7d94ac08d81121913d06a053a1d",
        "colab_type": "text",
        "id": "5N9MdkBbi_OT"
      },
      "source": [
        "# Step 4: Define the model\n",
        "\n",
        "#YOLO V1 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHPs7Ui0C4Ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def yolov1(pretrained_weights = None,input_size = (544,480,1)):\n",
        "\n",
        "  # Convolution block 0\n",
        "  inputs = Input(input_size)\n",
        "  conv0 = Conv2D(16,7,strides=(2, 2),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "  BN0=BatchNormalization()(conv0)\n",
        "  pool0=MaxPool2D(pool_size=(2, 2),strides=(2, 2))(BN0)\n",
        "\n",
        "  # Convolution block 1\n",
        "  conv1 = Conv2D(32,3,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(pool0)\n",
        "  BN1=BatchNormalization()(conv1)\n",
        "  pool1=MaxPool2D(pool_size=(2, 2),strides=(2, 2))(BN1)\n",
        "\n",
        "  # Convolution block 2\n",
        "  conv2 = Conv2D(16,1,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "  conv2 = Conv2D(32,3,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "  conv2 = Conv2D(16,1,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "  conv2 = Conv2D(32,3,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "  BN2=BatchNormalization()(conv2)\n",
        "  merge2=add([pool1,BN2])\n",
        "  pool2=MaxPool2D(pool_size=(2, 2),strides=(2, 2))(merge2)\n",
        "  pool2 = Conv2D(128,3,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "  \n",
        "\n",
        "  # Convolution block 3\n",
        "  conv3 = Conv2D(32,1,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "  conv3 = Conv2D(64,3,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "  conv3 = Conv2D(32,1,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "  conv3 = Conv2D(64,3,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "  conv3 = Conv2D(32,1,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "  conv3 = Conv2D(64,3,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "  conv3 = Conv2D(32,1,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "  conv3 = Conv2D(64,3,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "  conv3 = Conv2D(64,1,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "  conv3 = Conv2D(128,3,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "  BN3=BatchNormalization()(conv3)\n",
        "  merge3=add([pool2,BN3])\n",
        "  pool3=MaxPool2D(pool_size=(2, 2),strides=(2, 2))(merge3)\n",
        "  pool3_p = Conv2D(256,3,strides=(2, 2),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "\n",
        "  # Convolution block 4\n",
        "  conv4 = Conv2D(128,1,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "  conv4 = Conv2D(256,3,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "  conv4 = Conv2D(128,1,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "  conv4 = Conv2D(256,3,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "  conv4 = Conv2D(256,3,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "  conv4 = Conv2D(256,3,strides=(2, 2),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "  BN4=BatchNormalization()(conv4)  \n",
        "  merge4=add([pool3_p,BN4])\n",
        "\n",
        "  # Convolution block 5\n",
        "  conv5 = Conv2D(256,3,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(merge4)\n",
        "  conv5 = Conv2D(256,3,strides=(1, 1),activation =keras.layers.LeakyReLU(alpha=0.1), padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "  BN5=BatchNormalization()(conv5) \n",
        "  merge5=add([merge4,BN5])\n",
        "\n",
        "  #flatten and softmax\n",
        "  dense=Flatten()(merge5)\n",
        "  # dense=Dense(units=256,activation='relu')(dense)\n",
        "  dense=Dense(units=128,activation='relu')(dense)\n",
        "  dense=Dense(units=1024,activation='relu')(dense)\n",
        "  # dropout=Dropout(0.2)(dense)\n",
        "  coordinate=Dense(units=2, activation='linear')(dense)\n",
        "\n",
        "  model = Model(inputs =inputs ,outputs = coordinate)\n",
        "\n",
        "\n",
        "  # Define the optimizer\n",
        "  keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "  # Compile the model\n",
        "  model.compile(loss=euclidean_distance,optimizer='Adam', metrics=[\"accuracy\"])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "92f3186f6f1106f3ec491d99313b506eb514f011",
        "colab_type": "text",
        "id": "d2zi4rgZi_O_"
      },
      "source": [
        "# Step 5: data augmentation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s8LUgbiYcYoj",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=False,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = False, # Randomly zoom image\n",
        "        width_shift_range=False,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=False,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=True)  # randomly flip images\n",
        "\n",
        "datagen.fit(x_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6e983d68b0a343bdf313ccb70c7cd38afd32c89b",
        "colab_type": "text",
        "id": "Bq0JLDJji_PV"
      },
      "source": [
        "# Step 6 : Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3fe1228e7657f49d7323adc36c865790e9ac05eb",
        "colab_type": "code",
        "id": "QhF-jiF9i_PW",
        "outputId": "c2c9fd52-65c1-476b-b17b-d0d5ea9ebc3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model=yolov1()\n",
        "epochs = 200\n",
        "batch_size = 5\n",
        "model_checkpoint = ModelCheckpoint('cnn_regression.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
        "history = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n",
        "                              epochs = epochs, validation_data = (x_validate,y_validate),\n",
        "                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 83.6756 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 10s 203ms/step - loss: 83.0157 - acc: 1.0000 - val_loss: 421.0966 - val_acc: 1.0000\n",
            "Epoch 2/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 52.0511 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 52.0901 - acc: 1.0000 - val_loss: 226.1447 - val_acc: 1.0000\n",
            "Epoch 3/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 37.4747 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 37.2814 - acc: 1.0000 - val_loss: 77.0036 - val_acc: 1.0000\n",
            "Epoch 4/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 37.1081 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 37.2778 - acc: 1.0000 - val_loss: 45.7848 - val_acc: 1.0000\n",
            "Epoch 5/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 47.0644 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 46.5274 - acc: 1.0000 - val_loss: 27.4683 - val_acc: 1.0000\n",
            "Epoch 6/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 29.7173 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 29.8579 - acc: 1.0000 - val_loss: 23.8581 - val_acc: 1.0000\n",
            "Epoch 7/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 31.0076 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 31.0024 - acc: 1.0000 - val_loss: 33.9975 - val_acc: 1.0000\n",
            "Epoch 8/200\n",
            "45/48 [===========================>..] - ETA: 0s - loss: 29.3251 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 29.7051 - acc: 1.0000 - val_loss: 70.9564 - val_acc: 1.0000\n",
            "Epoch 9/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 34.9878 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 34.6660 - acc: 1.0000 - val_loss: 52.7916 - val_acc: 1.0000\n",
            "Epoch 10/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 30.7233 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 30.5974 - acc: 1.0000 - val_loss: 34.3749 - val_acc: 1.0000\n",
            "Epoch 11/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 26.0116 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 25.8673 - acc: 1.0000 - val_loss: 37.4993 - val_acc: 1.0000\n",
            "Epoch 12/200\n",
            "45/48 [===========================>..] - ETA: 0s - loss: 26.7157 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 26.4321 - acc: 1.0000 - val_loss: 28.5364 - val_acc: 1.0000\n",
            "Epoch 13/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 27.0501 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 27.2193 - acc: 1.0000 - val_loss: 28.5341 - val_acc: 1.0000\n",
            "Epoch 14/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 22.1722 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 22.0979 - acc: 1.0000 - val_loss: 23.8518 - val_acc: 1.0000\n",
            "Epoch 15/200\n",
            "45/48 [===========================>..] - ETA: 0s - loss: 30.8274 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 30.6264 - acc: 1.0000 - val_loss: 25.5913 - val_acc: 1.0000\n",
            "Epoch 16/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 27.1241 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 27.2596 - acc: 1.0000 - val_loss: 21.0976 - val_acc: 1.0000\n",
            "Epoch 17/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 22.0962 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 22.1562 - acc: 1.0000 - val_loss: 26.9316 - val_acc: 1.0000\n",
            "Epoch 18/200\n",
            "45/48 [===========================>..] - ETA: 0s - loss: 23.3965 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 22.9976 - acc: 1.0000 - val_loss: 27.1102 - val_acc: 1.0000\n",
            "Epoch 19/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 20.8770 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 20.9292 - acc: 1.0000 - val_loss: 19.9743 - val_acc: 1.0000\n",
            "Epoch 20/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 19.6624 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 19.5112 - acc: 1.0000 - val_loss: 17.8949 - val_acc: 1.0000\n",
            "Epoch 21/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 22.0131 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 22.0321 - acc: 1.0000 - val_loss: 17.7793 - val_acc: 1.0000\n",
            "Epoch 22/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 32.1197 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 31.6826 - acc: 1.0000 - val_loss: 78.8368 - val_acc: 1.0000\n",
            "Epoch 23/200\n",
            "45/48 [===========================>..] - ETA: 0s - loss: 21.2617 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 20.9051 - acc: 1.0000 - val_loss: 41.9560 - val_acc: 1.0000\n",
            "Epoch 24/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 19.5734 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 19.6314 - acc: 1.0000 - val_loss: 39.5447 - val_acc: 1.0000\n",
            "Epoch 25/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 20.0422 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 20.7929 - acc: 1.0000 - val_loss: 28.1715 - val_acc: 1.0000\n",
            "Epoch 26/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 21.0740 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 21.0415 - acc: 1.0000 - val_loss: 20.6778 - val_acc: 1.0000\n",
            "Epoch 27/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 21.0621 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 20.9448 - acc: 1.0000 - val_loss: 21.6307 - val_acc: 1.0000\n",
            "Epoch 28/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 17.2305 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 17.3565 - acc: 1.0000 - val_loss: 15.2165 - val_acc: 1.0000\n",
            "Epoch 29/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 16.9822 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 16.9146 - acc: 1.0000 - val_loss: 23.5487 - val_acc: 1.0000\n",
            "Epoch 30/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 19.0340 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 18.6596 - acc: 1.0000 - val_loss: 13.1928 - val_acc: 1.0000\n",
            "Epoch 31/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 14.8861 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 14.8357 - acc: 1.0000 - val_loss: 23.9218 - val_acc: 1.0000\n",
            "Epoch 32/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 18.5676 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 18.4398 - acc: 1.0000 - val_loss: 19.5745 - val_acc: 1.0000\n",
            "Epoch 33/200\n",
            "45/48 [===========================>..] - ETA: 0s - loss: 16.8790 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 16.7461 - acc: 1.0000 - val_loss: 14.1081 - val_acc: 1.0000\n",
            "Epoch 34/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 20.1566 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 19.9514 - acc: 1.0000 - val_loss: 26.3723 - val_acc: 1.0000\n",
            "Epoch 35/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 22.6698 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 22.7505 - acc: 1.0000 - val_loss: 28.4852 - val_acc: 1.0000\n",
            "Epoch 36/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 16.2274 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 16.3598 - acc: 1.0000 - val_loss: 30.0382 - val_acc: 1.0000\n",
            "Epoch 37/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 15.6418 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 15.5020 - acc: 1.0000 - val_loss: 17.8607 - val_acc: 1.0000\n",
            "Epoch 38/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 15.8023 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 16.1015 - acc: 1.0000 - val_loss: 24.0039 - val_acc: 1.0000\n",
            "Epoch 39/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 22.6514 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 22.4454 - acc: 1.0000 - val_loss: 40.3512 - val_acc: 1.0000\n",
            "Epoch 40/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 16.9092 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 16.8247 - acc: 1.0000 - val_loss: 14.6787 - val_acc: 1.0000\n",
            "Epoch 41/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 16.5327 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 16.5476 - acc: 1.0000 - val_loss: 22.6109 - val_acc: 1.0000\n",
            "Epoch 42/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 18.0090 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 17.6956 - acc: 1.0000 - val_loss: 27.9557 - val_acc: 1.0000\n",
            "Epoch 43/200\n",
            "45/48 [===========================>..] - ETA: 0s - loss: 18.2407 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 18.2340 - acc: 1.0000 - val_loss: 14.0504 - val_acc: 1.0000\n",
            "Epoch 44/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 17.9799 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 17.9160 - acc: 1.0000 - val_loss: 23.0305 - val_acc: 1.0000\n",
            "Epoch 45/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 16.4627 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 16.8745 - acc: 1.0000 - val_loss: 27.6626 - val_acc: 1.0000\n",
            "Epoch 46/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 17.3847 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 17.4169 - acc: 1.0000 - val_loss: 19.3304 - val_acc: 1.0000\n",
            "Epoch 47/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 15.5549 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 15.6426 - acc: 1.0000 - val_loss: 19.2848 - val_acc: 1.0000\n",
            "Epoch 48/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 16.6278 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 16.5997 - acc: 1.0000 - val_loss: 30.9991 - val_acc: 1.0000\n",
            "Epoch 49/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 14.6555 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 14.5555 - acc: 1.0000 - val_loss: 19.2309 - val_acc: 1.0000\n",
            "Epoch 50/200\n",
            "45/48 [===========================>..] - ETA: 0s - loss: 17.0596 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 17.1184 - acc: 1.0000 - val_loss: 25.9853 - val_acc: 1.0000\n",
            "Epoch 51/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 15.5193 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 15.5596 - acc: 1.0000 - val_loss: 37.0078 - val_acc: 1.0000\n",
            "Epoch 52/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 16.6362 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 16.5583 - acc: 1.0000 - val_loss: 16.5417 - val_acc: 1.0000\n",
            "Epoch 53/200\n",
            "45/48 [===========================>..] - ETA: 0s - loss: 13.9488 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 13.6421 - acc: 1.0000 - val_loss: 17.7444 - val_acc: 1.0000\n",
            "Epoch 54/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 14.9907 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 15.6966 - acc: 1.0000 - val_loss: 18.4740 - val_acc: 1.0000\n",
            "Epoch 55/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 14.5132 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 14.6395 - acc: 1.0000 - val_loss: 11.3444 - val_acc: 1.0000\n",
            "Epoch 56/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 15.8045 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 16.0771 - acc: 1.0000 - val_loss: 21.1236 - val_acc: 1.0000\n",
            "Epoch 57/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 17.6572 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 17.5602 - acc: 1.0000 - val_loss: 20.4671 - val_acc: 1.0000\n",
            "Epoch 58/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 12.7866 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 12.7183 - acc: 1.0000 - val_loss: 15.3088 - val_acc: 1.0000\n",
            "Epoch 59/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 12.0626 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 12.3493 - acc: 1.0000 - val_loss: 23.0489 - val_acc: 1.0000\n",
            "Epoch 60/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 13.3619 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 13.4687 - acc: 1.0000 - val_loss: 23.3514 - val_acc: 1.0000\n",
            "Epoch 61/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 14.3125 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 14.1828 - acc: 1.0000 - val_loss: 25.4423 - val_acc: 1.0000\n",
            "Epoch 62/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 14.7314 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 14.7653 - acc: 1.0000 - val_loss: 13.5645 - val_acc: 1.0000\n",
            "Epoch 63/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 13.3991 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 13.3262 - acc: 1.0000 - val_loss: 15.5591 - val_acc: 1.0000\n",
            "Epoch 64/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 11.3011 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 11.4995 - acc: 1.0000 - val_loss: 24.8798 - val_acc: 1.0000\n",
            "Epoch 65/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 13.9814 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 13.8780 - acc: 1.0000 - val_loss: 16.2655 - val_acc: 1.0000\n",
            "Epoch 66/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 12.3429 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 12.3261 - acc: 1.0000 - val_loss: 17.9332 - val_acc: 1.0000\n",
            "Epoch 67/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 15.5605 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 15.5636 - acc: 1.0000 - val_loss: 15.3094 - val_acc: 1.0000\n",
            "Epoch 68/200\n",
            "45/48 [===========================>..] - ETA: 0s - loss: 22.2406 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 21.8864 - acc: 1.0000 - val_loss: 14.9247 - val_acc: 1.0000\n",
            "Epoch 69/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 14.8870 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 14.5855 - acc: 1.0000 - val_loss: 29.9557 - val_acc: 1.0000\n",
            "Epoch 70/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 11.8010 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 11.7987 - acc: 1.0000 - val_loss: 18.3981 - val_acc: 1.0000\n",
            "Epoch 71/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 12.9486 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 13.4209 - acc: 1.0000 - val_loss: 15.4578 - val_acc: 1.0000\n",
            "Epoch 72/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 13.5457 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 13.4659 - acc: 1.0000 - val_loss: 10.3780 - val_acc: 1.0000\n",
            "Epoch 73/200\n",
            "45/48 [===========================>..] - ETA: 0s - loss: 10.1268 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 10.1078 - acc: 1.0000 - val_loss: 21.7915 - val_acc: 1.0000\n",
            "Epoch 74/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 14.9464 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 14.8320 - acc: 1.0000 - val_loss: 14.3670 - val_acc: 1.0000\n",
            "Epoch 75/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 15.7167 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 15.6653 - acc: 1.0000 - val_loss: 16.0188 - val_acc: 1.0000\n",
            "Epoch 76/200\n",
            "45/48 [===========================>..] - ETA: 0s - loss: 13.8486 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 13.6079 - acc: 1.0000 - val_loss: 18.5446 - val_acc: 1.0000\n",
            "Epoch 77/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 17.0056 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 16.8426 - acc: 1.0000 - val_loss: 19.1602 - val_acc: 1.0000\n",
            "Epoch 78/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 12.1803 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 12.1210 - acc: 1.0000 - val_loss: 16.7768 - val_acc: 1.0000\n",
            "Epoch 79/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 10.3943 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 10.4064 - acc: 1.0000 - val_loss: 14.3823 - val_acc: 1.0000\n",
            "Epoch 80/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 12.2518 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 12.2497 - acc: 1.0000 - val_loss: 11.2553 - val_acc: 1.0000\n",
            "Epoch 81/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 11.0903 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 11.1187 - acc: 1.0000 - val_loss: 17.8403 - val_acc: 1.0000\n",
            "Epoch 82/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 15.0493 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 14.7667 - acc: 1.0000 - val_loss: 15.5404 - val_acc: 1.0000\n",
            "Epoch 83/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.6964 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 11.8715 - acc: 1.0000 - val_loss: 14.7433 - val_acc: 1.0000\n",
            "Epoch 84/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 16.5232 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 2s 32ms/step - loss: 16.3808 - acc: 1.0000 - val_loss: 12.4376 - val_acc: 1.0000\n",
            "Epoch 85/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 13.5541 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 2s 32ms/step - loss: 13.4998 - acc: 1.0000 - val_loss: 21.6456 - val_acc: 1.0000\n",
            "Epoch 86/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 11.5486 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 11.5958 - acc: 1.0000 - val_loss: 16.7131 - val_acc: 1.0000\n",
            "Epoch 87/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 10.8465 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 10.7640 - acc: 1.0000 - val_loss: 16.3702 - val_acc: 1.0000\n",
            "Epoch 88/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 12.5917 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 12.4641 - acc: 1.0000 - val_loss: 21.0166 - val_acc: 1.0000\n",
            "Epoch 89/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.8995 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 11.9123 - acc: 1.0000 - val_loss: 17.4656 - val_acc: 1.0000\n",
            "Epoch 90/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 10.3637 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 10.3285 - acc: 1.0000 - val_loss: 16.6573 - val_acc: 1.0000\n",
            "Epoch 91/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 10.1574 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 10.2329 - acc: 1.0000 - val_loss: 10.5399 - val_acc: 1.0000\n",
            "Epoch 92/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.8349 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 11.9361 - acc: 1.0000 - val_loss: 14.0645 - val_acc: 1.0000\n",
            "Epoch 93/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 12.2582 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 12.2267 - acc: 1.0000 - val_loss: 12.6932 - val_acc: 1.0000\n",
            "Epoch 94/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.6823 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 9.8229 - acc: 1.0000 - val_loss: 10.8635 - val_acc: 1.0000\n",
            "Epoch 95/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 13.8788 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 13.7035 - acc: 1.0000 - val_loss: 14.9601 - val_acc: 1.0000\n",
            "Epoch 96/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 10.1980 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 10.2508 - acc: 1.0000 - val_loss: 21.7444 - val_acc: 1.0000\n",
            "Epoch 97/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.0557 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 11.0326 - acc: 1.0000 - val_loss: 14.2165 - val_acc: 1.0000\n",
            "Epoch 98/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 10.7942 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 10.8121 - acc: 1.0000 - val_loss: 9.8632 - val_acc: 1.0000\n",
            "Epoch 99/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 11.1618 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 11.0414 - acc: 1.0000 - val_loss: 13.3473 - val_acc: 1.0000\n",
            "Epoch 100/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 15.8294 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 15.9137 - acc: 1.0000 - val_loss: 18.9372 - val_acc: 1.0000\n",
            "Epoch 101/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 12.6091 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 12.6407 - acc: 1.0000 - val_loss: 10.8334 - val_acc: 1.0000\n",
            "Epoch 102/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.1252 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 11.0080 - acc: 1.0000 - val_loss: 9.6477 - val_acc: 1.0000\n",
            "Epoch 103/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 10.3288 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 10.4748 - acc: 1.0000 - val_loss: 11.3071 - val_acc: 1.0000\n",
            "Epoch 104/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 13.1147 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 13.0969 - acc: 1.0000 - val_loss: 9.7660 - val_acc: 1.0000\n",
            "Epoch 105/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.4852 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 11.5180 - acc: 1.0000 - val_loss: 18.2379 - val_acc: 1.0000\n",
            "Epoch 106/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 13.7729 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 2s 32ms/step - loss: 13.8678 - acc: 1.0000 - val_loss: 26.8668 - val_acc: 1.0000\n",
            "Epoch 107/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.5505 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 2s 31ms/step - loss: 9.5686 - acc: 1.0000 - val_loss: 15.9697 - val_acc: 1.0000\n",
            "Epoch 108/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.9407 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 8.8912 - acc: 1.0000 - val_loss: 12.7935 - val_acc: 1.0000\n",
            "Epoch 109/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 10.3288 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 10.3420 - acc: 1.0000 - val_loss: 11.2565 - val_acc: 1.0000\n",
            "Epoch 110/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.5633 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 11.6570 - acc: 1.0000 - val_loss: 13.8455 - val_acc: 1.0000\n",
            "Epoch 111/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 11.8049 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 12.0235 - acc: 1.0000 - val_loss: 11.9075 - val_acc: 1.0000\n",
            "Epoch 112/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.8021 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 11.7708 - acc: 1.0000 - val_loss: 12.9442 - val_acc: 1.0000\n",
            "Epoch 113/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.9273 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 7.9281 - acc: 1.0000 - val_loss: 11.5003 - val_acc: 1.0000\n",
            "Epoch 114/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.2738 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 9.2645 - acc: 1.0000 - val_loss: 12.2045 - val_acc: 1.0000\n",
            "Epoch 115/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 12.4642 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 12.4203 - acc: 1.0000 - val_loss: 12.0889 - val_acc: 1.0000\n",
            "Epoch 116/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.5320 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 9.4838 - acc: 1.0000 - val_loss: 15.6626 - val_acc: 1.0000\n",
            "Epoch 117/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.3306 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 9.3422 - acc: 1.0000 - val_loss: 13.6849 - val_acc: 1.0000\n",
            "Epoch 118/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.8774 - acc: 1.0000 Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 9.9414 - acc: 1.0000 - val_loss: 18.5029 - val_acc: 1.0000\n",
            "Epoch 119/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.7563 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 11.6177 - acc: 1.0000 - val_loss: 13.9908 - val_acc: 1.0000\n",
            "Epoch 120/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.7601 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 11.9347 - acc: 1.0000 - val_loss: 20.2740 - val_acc: 1.0000\n",
            "Epoch 121/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 10.9659 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 10.9954 - acc: 1.0000 - val_loss: 23.4169 - val_acc: 1.0000\n",
            "Epoch 122/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 11.5873 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 11.8476 - acc: 1.0000 - val_loss: 10.6409 - val_acc: 1.0000\n",
            "Epoch 123/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.4124 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 8.4245 - acc: 1.0000 - val_loss: 10.3736 - val_acc: 1.0000\n",
            "Epoch 124/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.7077 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 11.6360 - acc: 1.0000 - val_loss: 21.2926 - val_acc: 1.0000\n",
            "Epoch 125/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.7616 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 11.6407 - acc: 1.0000 - val_loss: 10.0260 - val_acc: 1.0000\n",
            "Epoch 126/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 10.0075 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 9.9583 - acc: 1.0000 - val_loss: 14.5783 - val_acc: 1.0000\n",
            "Epoch 127/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 10.6064 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 10.5628 - acc: 1.0000 - val_loss: 10.1892 - val_acc: 1.0000\n",
            "Epoch 128/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.6643 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 8.7682 - acc: 1.0000 - val_loss: 12.5048 - val_acc: 1.0000\n",
            "Epoch 129/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 12.0793 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 11.9716 - acc: 1.0000 - val_loss: 10.3949 - val_acc: 1.0000\n",
            "Epoch 130/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 10.3846 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 10.2905 - acc: 1.0000 - val_loss: 9.8176 - val_acc: 1.0000\n",
            "Epoch 131/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.4913 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 7.4774 - acc: 1.0000 - val_loss: 13.9209 - val_acc: 1.0000\n",
            "Epoch 132/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.9183 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 10.0238 - acc: 1.0000 - val_loss: 10.0392 - val_acc: 1.0000\n",
            "Epoch 133/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.3025 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 8.2576 - acc: 1.0000 - val_loss: 12.6623 - val_acc: 1.0000\n",
            "Epoch 134/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.6395 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 8.5713 - acc: 1.0000 - val_loss: 12.5000 - val_acc: 1.0000\n",
            "Epoch 135/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.6865 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 7.6336 - acc: 1.0000 - val_loss: 9.3587 - val_acc: 1.0000\n",
            "Epoch 136/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.9773 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 9.0659 - acc: 1.0000 - val_loss: 15.7916 - val_acc: 1.0000\n",
            "Epoch 137/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.0294 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 10.9976 - acc: 1.0000 - val_loss: 10.0244 - val_acc: 1.0000\n",
            "Epoch 138/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.1422 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 8.1801 - acc: 1.0000 - val_loss: 10.2073 - val_acc: 1.0000\n",
            "Epoch 139/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.7056 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 8.6830 - acc: 1.0000 - val_loss: 10.0894 - val_acc: 1.0000\n",
            "Epoch 140/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.5880 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 8.5766 - acc: 1.0000 - val_loss: 14.9934 - val_acc: 1.0000\n",
            "Epoch 141/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.6172 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 8.6417 - acc: 1.0000 - val_loss: 11.6961 - val_acc: 1.0000\n",
            "Epoch 142/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.3846 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 9.3492 - acc: 1.0000 - val_loss: 10.7596 - val_acc: 1.0000\n",
            "Epoch 143/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.6883 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 8.5753 - acc: 1.0000 - val_loss: 17.2107 - val_acc: 1.0000\n",
            "Epoch 144/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.8058 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 7.8258 - acc: 1.0000 - val_loss: 13.7333 - val_acc: 1.0000\n",
            "Epoch 145/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.8624 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 8.9000 - acc: 1.0000 - val_loss: 22.8686 - val_acc: 1.0000\n",
            "Epoch 146/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.8510 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 8.7540 - acc: 1.0000 - val_loss: 13.9519 - val_acc: 1.0000\n",
            "Epoch 147/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.2811 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 9.4975 - acc: 1.0000 - val_loss: 25.4436 - val_acc: 1.0000\n",
            "Epoch 148/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.5238 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 9.4480 - acc: 1.0000 - val_loss: 10.4255 - val_acc: 1.0000\n",
            "Epoch 149/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.9180 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 7.9256 - acc: 1.0000 - val_loss: 15.7588 - val_acc: 1.0000\n",
            "Epoch 150/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.5634 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 8.5760 - acc: 1.0000 - val_loss: 11.5034 - val_acc: 1.0000\n",
            "Epoch 151/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.8139 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 8.9449 - acc: 1.0000 - val_loss: 15.5959 - val_acc: 1.0000\n",
            "Epoch 152/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.1440 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 8.0887 - acc: 1.0000 - val_loss: 8.1210 - val_acc: 1.0000\n",
            "Epoch 153/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.8976 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 11.8420 - acc: 1.0000 - val_loss: 14.9613 - val_acc: 1.0000\n",
            "Epoch 154/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.7253 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 11.5701 - acc: 1.0000 - val_loss: 11.4257 - val_acc: 1.0000\n",
            "Epoch 155/200\n",
            "45/48 [===========================>..] - ETA: 0s - loss: 8.9085 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 8.6578 - acc: 1.0000 - val_loss: 14.0635 - val_acc: 1.0000\n",
            "Epoch 156/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.1218 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 9.1066 - acc: 1.0000 - val_loss: 13.3805 - val_acc: 1.0000\n",
            "Epoch 157/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.6735 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 7.5888 - acc: 1.0000 - val_loss: 12.7038 - val_acc: 1.0000\n",
            "Epoch 158/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 6.9612 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 6.9702 - acc: 1.0000 - val_loss: 10.5595 - val_acc: 1.0000\n",
            "Epoch 159/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.9599 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 7.9299 - acc: 1.0000 - val_loss: 11.9136 - val_acc: 1.0000\n",
            "Epoch 160/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.8713 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 9.9554 - acc: 1.0000 - val_loss: 20.7122 - val_acc: 1.0000\n",
            "Epoch 161/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.1914 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 9.0934 - acc: 1.0000 - val_loss: 15.5457 - val_acc: 1.0000\n",
            "Epoch 162/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 10.5864 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 2s 31ms/step - loss: 10.6192 - acc: 1.0000 - val_loss: 22.2510 - val_acc: 1.0000\n",
            "Epoch 163/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.2326 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 8.1723 - acc: 1.0000 - val_loss: 7.4738 - val_acc: 1.0000\n",
            "Epoch 164/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.2337 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 8.2690 - acc: 1.0000 - val_loss: 16.6028 - val_acc: 1.0000\n",
            "Epoch 165/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.8204 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 9.8890 - acc: 1.0000 - val_loss: 10.6563 - val_acc: 1.0000\n",
            "Epoch 166/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.8926 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 8.8844 - acc: 1.0000 - val_loss: 11.2312 - val_acc: 1.0000\n",
            "Epoch 167/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 6.9155 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 6.9254 - acc: 1.0000 - val_loss: 11.2843 - val_acc: 1.0000\n",
            "Epoch 168/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.2151 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 7.1461 - acc: 1.0000 - val_loss: 11.5842 - val_acc: 1.0000\n",
            "Epoch 169/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.5792 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 8.6184 - acc: 1.0000 - val_loss: 14.1100 - val_acc: 1.0000\n",
            "Epoch 170/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.7335 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 9.7435 - acc: 1.0000 - val_loss: 10.8547 - val_acc: 1.0000\n",
            "Epoch 171/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.8997 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 9.8044 - acc: 1.0000 - val_loss: 11.2141 - val_acc: 1.0000\n",
            "Epoch 172/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 10.6965 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 10.7452 - acc: 1.0000 - val_loss: 9.2414 - val_acc: 1.0000\n",
            "Epoch 173/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 10.1689 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 10.1694 - acc: 1.0000 - val_loss: 11.6647 - val_acc: 1.0000\n",
            "Epoch 174/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.4194 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 8.4239 - acc: 1.0000 - val_loss: 10.8302 - val_acc: 1.0000\n",
            "Epoch 175/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 11.0684 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 10.9472 - acc: 1.0000 - val_loss: 11.6285 - val_acc: 1.0000\n",
            "Epoch 176/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.6193 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 8.7458 - acc: 1.0000 - val_loss: 10.9279 - val_acc: 1.0000\n",
            "Epoch 177/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.5839 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 9.4963 - acc: 1.0000 - val_loss: 9.5858 - val_acc: 1.0000\n",
            "Epoch 178/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.8266 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 9.1445 - acc: 1.0000 - val_loss: 11.2990 - val_acc: 1.0000\n",
            "Epoch 179/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.3835 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 2s 31ms/step - loss: 9.2532 - acc: 1.0000 - val_loss: 8.9480 - val_acc: 1.0000\n",
            "Epoch 180/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.6892 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 2s 32ms/step - loss: 9.5897 - acc: 1.0000 - val_loss: 19.4238 - val_acc: 1.0000\n",
            "Epoch 181/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.9289 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 2s 31ms/step - loss: 7.9844 - acc: 1.0000 - val_loss: 14.8281 - val_acc: 1.0000\n",
            "Epoch 182/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.2579 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 7.2103 - acc: 1.0000 - val_loss: 9.6380 - val_acc: 1.0000\n",
            "Epoch 183/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.7547 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 9.7095 - acc: 1.0000 - val_loss: 12.8578 - val_acc: 1.0000\n",
            "Epoch 184/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.6522 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 7.6402 - acc: 1.0000 - val_loss: 15.4893 - val_acc: 1.0000\n",
            "Epoch 185/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 6.8021 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 6.8410 - acc: 1.0000 - val_loss: 10.1881 - val_acc: 1.0000\n",
            "Epoch 186/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.4896 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 2s 31ms/step - loss: 8.5509 - acc: 1.0000 - val_loss: 18.2721 - val_acc: 1.0000\n",
            "Epoch 187/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 6.6937 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 6.7043 - acc: 1.0000 - val_loss: 9.0116 - val_acc: 1.0000\n",
            "Epoch 188/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.7001 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 7.6790 - acc: 1.0000 - val_loss: 8.4265 - val_acc: 1.0000\n",
            "Epoch 189/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 10.1232 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 9.9875 - acc: 1.0000 - val_loss: 9.7502 - val_acc: 1.0000\n",
            "Epoch 190/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 9.4764 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 9.5117 - acc: 1.0000 - val_loss: 13.0700 - val_acc: 1.0000\n",
            "Epoch 191/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.9720 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 7.9750 - acc: 1.0000 - val_loss: 9.6284 - val_acc: 1.0000\n",
            "Epoch 192/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 6.9431 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 6.9375 - acc: 1.0000 - val_loss: 16.4216 - val_acc: 1.0000\n",
            "Epoch 193/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.3123 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 7.3468 - acc: 1.0000 - val_loss: 8.9900 - val_acc: 1.0000\n",
            "Epoch 194/200\n",
            "46/48 [===========================>..] - ETA: 0s - loss: 8.9942 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 9.0007 - acc: 1.0000 - val_loss: 8.8088 - val_acc: 1.0000\n",
            "Epoch 195/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.8743 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 8.8260 - acc: 1.0000 - val_loss: 15.2721 - val_acc: 1.0000\n",
            "Epoch 196/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.0254 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 7.0820 - acc: 1.0000 - val_loss: 10.4282 - val_acc: 1.0000\n",
            "Epoch 197/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.5964 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 7.6414 - acc: 1.0000 - val_loss: 22.6590 - val_acc: 1.0000\n",
            "Epoch 198/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.1716 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 8.1341 - acc: 1.0000 - val_loss: 10.4375 - val_acc: 1.0000\n",
            "Epoch 199/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 7.9926 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 7.9259 - acc: 1.0000 - val_loss: 13.8679 - val_acc: 1.0000\n",
            "Epoch 200/200\n",
            "47/48 [============================>.] - ETA: 0s - loss: 8.0098 - acc: 1.0000Epoch 1/200\n",
            "48/48 [==============================] - 1s 31ms/step - loss: 8.0496 - acc: 1.0000 - val_loss: 12.7427 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2d1c806e6c6e46916ffb40b5e2848c66c33ed719",
        "colab_type": "text",
        "id": "sCcUy20ki_Pa"
      },
      "source": [
        "# Step 7 : History visulization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "cd19d9fa10edf4cd89f178db0291be76dbdcbfef",
        "colab_type": "code",
        "id": "U2y4NEsxi_Pe",
        "outputId": "96cdd08d-523b-44fa-bfc6-3bd18038e1ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('CNN model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "857f705a561f046a1d63ffa17a8a0b1e8da16ff5",
        "colab_type": "text",
        "id": "PJUE121qi_Ps"
      },
      "source": [
        "# Step 8: Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2d1658a1c2ea5379e1e2064f7fcd5ff1313046a4",
        "colab_type": "code",
        "id": "jD72Ohxji_Pv",
        "outputId": "f738bb1b-0854-4801-c4e6-648a28b423b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#euclidean distance\n",
        "result=model.predict(x_test)\n",
        "error=result-y_test\n",
        "distance=[]\n",
        "for i in range(error.shape[0]):\n",
        "    ed=np.sqrt(error[i,0]**2+error[i,1]**2)\n",
        "    distance.append(ed)\n",
        "distance=np.array(distance)\n",
        "print(\"overall mean distance error is\",np.mean(distance))\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "overall mean distance error is 11.601940859279523\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}